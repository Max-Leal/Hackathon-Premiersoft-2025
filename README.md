Com certeza! A seguir, apresento uma versÃ£o do seu README formatada em Markdown, ideal para o GitHub. Ela corrige a estrutura, melhora a legibilidade e adiciona elementos como uma tabela de conteÃºdos para facilitar a navegaÃ§Ã£o.

APS Hackathon Premiersoft 2025
Sistema de gestÃ£o e anÃ¡lise de dados para saÃºde pÃºblica, com dashboard interativo, ingestÃ£o de dados, alocaÃ§Ã£o inteligente de recursos e consulta de entidades.

ğŸ“œ Tabela de ConteÃºdos
ğŸ“‚ Estrutura do Projeto

ğŸš€ Como Rodar o Projeto

ğŸ“Š Funcionalidades

ğŸ›ï¸ Arquitetura da SoluÃ§Ã£o

ğŸ› ï¸ Requisitos

ğŸ“Œ ObservaÃ§Ãµes

âœï¸ Autores

ğŸ“‚ Estrutura do Projeto
A estrutura de pastas foi organizada para separar as responsabilidades e facilitar a manutenÃ§Ã£o.

â”œâ”€â”€ data/              # Dados brutos e scripts de inicializaÃ§Ã£o
â”œâ”€â”€ scripts/           # Scripts SQL para inicializaÃ§Ã£o do banco
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ common/        # FunÃ§Ãµes de utilidade (ex: cÃ¡lculos geogrÃ¡ficos)
â”‚   â”œâ”€â”€ core/          # LÃ³gica de negÃ³cio pura (ex: algoritmos de alocaÃ§Ã£o)
â”‚   â”œâ”€â”€ infrastructure/  # ConexÃ£o e interaÃ§Ã£o com o banco de dados
â”‚   â”œâ”€â”€ ingestion/     # Conversores para mÃºltiplos formatos de arquivo (CSV, XML, HL7)
â”‚   â”œâ”€â”€ pipeline/      # OrquestraÃ§Ã£o do ETL (Extract, Transform, Load)
â”‚   â””â”€â”€ frontend/      # Interface do dashboard (Streamlit)
â”œâ”€â”€ .streamlit/        # ConfiguraÃ§Ãµes do Streamlit
â”œâ”€â”€ docker-compose.yml # OrquestraÃ§Ã£o de containers
â””â”€â”€ requirements.txt   # DependÃªncias Python
ğŸš€ Como Rodar o Projeto
Siga os passos abaixo para executar a aplicaÃ§Ã£o em seu ambiente local.

PrÃ©-requisitos
Docker

Docker Compose

Python 3.11+

1. Clone o RepositÃ³rio
Bash

git clone <URL_DO_SEU_REPOSITORIO>
cd <NOME_DO_SEU_REPOSITORIO>
2. Suba os ServiÃ§os com Docker Compose
Este comando irÃ¡ construir as imagens, iniciar o banco de dados, executar o pipeline de ETL e, em seguida, subir a aplicaÃ§Ã£o web.

Bash

docker-compose up --build
Alternativamente, vocÃª pode executar os serviÃ§os passo a passo:

Bash

# ConstrÃ³i as imagens
docker-compose build

# Inicia apenas o banco de dados em background
docker-compose up -d db

# Executa o pipeline de ETL para popular o banco
docker-compose run pipeline

# Inicia todos os serviÃ§os (incluindo o frontend)
docker-compose up
3. Instale as DependÃªncias (se for rodar localmente sem Docker)
Caso prefira executar o frontend fora do container, certifique-se de que o banco de dados esteja rodando via Docker e instale as dependÃªncias:

Bash

pip install -r requirements.txt
4. Execute o Frontend (se for rodar localmente sem Docker)
Com as dependÃªncias instaladas, inicie a interface Streamlit:

Bash

streamlit run src/frontend/app.py
A aplicaÃ§Ã£o estarÃ¡ disponÃ­vel em http://localhost:8501.

ğŸ“Š Funcionalidades
Dashboard Interativo: VisualizaÃ§Ã£o dos principais indicadores de saÃºde, como ocupaÃ§Ã£o de leitos, distribuiÃ§Ã£o de profissionais e estatÃ­sticas epidemiolÃ³gicas.

IngestÃ£o de Dados: Sistema de upload robusto que aceita diversos formatos de dados brutos (Excel, CSV, XML, JSON, HL7).

AlocaÃ§Ã£o Inteligente: Algoritmos para otimizar a alocaÃ§Ã£o de mÃ©dicos e pacientes, sugerindo as melhores combinaÃ§Ãµes com base em localizaÃ§Ã£o, especialidade e capacidade.

Consulta de Entidades: Interface para busca, filtro e navegaÃ§Ã£o por entidades cadastradas (hospitais, mÃ©dicos, pacientes).

ğŸ›ï¸ Arquitetura da SoluÃ§Ã£o
O sistema foi desenhado com uma clara separaÃ§Ã£o de responsabilidades entre as camadas, facilitando a manutenÃ§Ã£o e a escalabilidade. O fluxo de dados segue as etapas clÃ¡ssicas de um pipeline de ETL.

Fluxo de Dados
IngestÃ£o (/src/ingestion): Arquivos de mÃºltiplos formatos (.csv, .xlsx, .hl7, etc.) sÃ£o lidos e convertidos para um schema padronizado em memÃ³ria (DataFrame Pandas). Um SCHEMA_MAP universal Ã© usado para traduzir os nomes das colunas de diferentes fontes.

TransformaÃ§Ã£o (/src/pipeline/transform.py): Os DataFrames padronizados passam por um processo de:

Limpeza de dados (valores nulos, formatos inconsistentes).

ValidaÃ§Ã£o de tipos (ex: leitos_totais para inteiro).

RemoÃ§Ã£o de duplicatas (ex: cpf de pacientes).

Enriquecimento (ex: adiÃ§Ã£o de coordenadas geogrÃ¡ficas a mÃ©dicos e hospitais).

LÃ³gica de NegÃ³cio (/src/core): FunÃ§Ãµes "puras" recebem os DataFrames limpos e aplicam as regras complexas de alocaÃ§Ã£o de mÃ©dicos e pacientes, retornando os resultados como novos DataFrames. Esta camada nÃ£o tem conhecimento sobre o banco de dados ou a origem dos dados.

Carga (/src/pipeline/load.py): A camada final recebe todos os DataFrames processados e os persiste no banco de dados PostgreSQL, respeitando a ordem de dependÃªncia das tabelas para garantir a integridade referencial.

VisualizaÃ§Ã£o (/src/frontend): O dashboard Streamlit lÃª os dados jÃ¡ consolidados e limpos diretamente do banco de dados para apresentar os KPIs e grÃ¡ficos interativos ao usuÃ¡rio final.

ğŸ› ï¸ Requisitos
Linguagem: Python 3.11

Processamento de Dados: Pandas

Banco de Dados: PostgreSQL + PostGIS

Dashboard: Streamlit

ContainerizaÃ§Ã£o: Docker & Docker Compose

Bibliotecas Principais: sqlalchemy, psycopg2-binary, python-hl7, geopandas

ğŸ“Œ ObservaÃ§Ãµes
Os dados de exemplo para teste estÃ£o localizados na pasta data/raw/.

O banco de dados Ã© inicializado com tabelas e tipos de dados customizados atravÃ©s do script scripts/init.sql.

As configuraÃ§Ãµes do tema e layout do Streamlit estÃ£o na pasta .streamlit/.

âœï¸ Autores
Projeto desenvolvido por Matheus Dias Estacio, Eric Dias, Max Augusto Leal e Leonardo Muller Mandel para o APS Hackathon Premiersoft 2025, com fins educacionais e de inovaÃ§Ã£o em saÃºde pÃºblica.
